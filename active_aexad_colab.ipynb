{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMo2JGOvmZaDN5jm4+unheg",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ileniagalati/active_ae_xad/blob/main/active_aexad_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-cC7PY1Wf5M",
    "outputId": "adf38f0f-ca59-45e9-8b2f-68ef20f86595",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Collegamento al drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AE_XAD/')\n",
    "'''"
   ],
   "metadata": {
    "id": "JLBVzcRWWtY8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "token=\"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\"\n",
    "repo_url = \"https://github.com/ileniagalati/active_ae_xad.git\"\n",
    "\n",
    "!git clone https://{token}@github.com/ileniagalati/active_ae_xad.git\n",
    "\n",
    "%cd active_ae_xad\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGfVvzK9WrgX",
    "outputId": "7228e431-f2ef-41f7-9df1-f37806bd2456",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'active_ae_xad'...\n",
      "remote: Enumerating objects: 1260, done.\u001B[K\n",
      "remote: Counting objects: 100% (1260/1260), done.\u001B[K\n",
      "remote: Compressing objects: 100% (1151/1151), done.\u001B[K\n",
      "remote: Total 1260 (delta 122), reused 1209 (delta 80), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (1260/1260), 29.11 MiB | 20.90 MiB/s, done.\n",
      "Resolving deltas: 100% (122/122), done.\n",
      "/content/active_ae_xad\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.argv = [\"active_launch.py\", \"-ds\", \"mvtec\", \"-budget\", \"2\"]\n",
    "\n",
    "# Ora esegui il resto del codice\n",
    "!python3 active_launch.py -ds mvtec -budget 2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wnFAULebwtW",
    "outputId": "2769d46b-541b-47a6-fea0-80da1128a746",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-11-01 16:36:29.757099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 16:36:29.776680: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 16:36:29.782665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 16:36:30.808482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "True\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/active_ae_xad/active_launch.py\", line 76, in <module>\n",
      "    mvtec(5,dataset_path,15)\n",
      "  File \"/content/active_ae_xad/aexad/tools/create_dataset.py\", line 250, in mvtec\n",
      "    normal_files_tr = os.listdir(f_path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'datasets/mvtec/hazelnut/train/good'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from aexad.tools.create_dataset import load_brainMRI_dataset\n",
    "from aexad.tools.create_dataset import mvtec\n",
    "from active_aexad_script import launch as launch_aexad\n",
    "import numpy as np\n",
    "from aexad.tools.utils import plot_image_tosave, plot_heatmap_tosave\n",
    "\n",
    "def f(x):\n",
    "    return 1-x\n",
    "\n",
    "def training_active_aexad(data_path,epochs,dataset,lambda_u, lambda_n, lambda_a):\n",
    "    heatmaps, scores, _, _, tot_time = launch_aexad(data_path, epochs, 16, 32, lambda_u, lambda_n, lambda_a, f=f, AE_type='conv',\n",
    "                                                    save_intermediate=True, save_path=ret_path,dataset=dataset,loss='aaexad')\n",
    "    np.save(open(os.path.join(ret_path, 'aexad_htmaps.npy'), 'wb'), heatmaps)\n",
    "    np.save(open(os.path.join(ret_path, 'aexad_scores.npy'), 'wb'), scores)\n",
    "    times.append(tot_time)\n",
    "    np.save(open(os.path.join(ret_path, 'times.npy'), 'wb'), np.array(times))\n",
    "\n",
    "    return heatmaps, scores, _, _, tot_time\n",
    "\n",
    "def run_mask_generation(from_path,to_path):\n",
    "    subprocess.run([\"python3\", \"MaskGenerator.py\" ,\"-from_path\",from_path,\"-to_path\",to_path])\n",
    "\n",
    "def update_datasets(image_idx, mask_array, X_train, Y_train, GT_train):\n",
    "\n",
    "    if np.sum(mask_array) > 0:\n",
    "        Y_train[image_idx] = -1\n",
    "        GT_train[image_idx] = mask_array\n",
    "    else:\n",
    "        Y_train[image_idx] = 1\n",
    "        GT_train[image_idx] = mask_array\n",
    "\n",
    "    n = len(X_train)\n",
    "    print(\"unlabeled examples: \", np.sum(Y_train == 0))\n",
    "    print(\"normal examples: \", np.sum(Y_train == 1))\n",
    "    print(\"anomalous examples: \", np.sum(Y_train == -1))\n",
    "\n",
    "    lambda_u = 1 / np.sum(Y_train == 0) if np.sum(Y_train == 0) > 0 else 0\n",
    "    lambda_n = 1 / np.sum(Y_train == 1) if np.sum(Y_train == 1) > 0 else 0\n",
    "    lambda_a = 1 / np.sum(Y_train == -1) if np.sum(Y_train == -1) > 0 else 0\n",
    "\n",
    "    print(\"unlabeled lambda: \", lambda_u)\n",
    "    print(\"normal lambda: \", lambda_n)\n",
    "    print(\"anomalous lambda: \", lambda_a)\n",
    "\n",
    "    return X_train, Y_train, GT_train, lambda_u, lambda_n, lambda_a\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    print(torch.cuda.is_available())\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-ds', type=str, help='Dataset to use')\n",
    "    parser.add_argument('-budget', type=int, help='Budget')\n",
    "    args = parser.parse_args()\n",
    "    b=args.budget\n",
    "\n",
    "    dataset_path= f'datasets/{args.ds}'\n",
    "\n",
    "    if args.ds == 'brain':\n",
    "        X_train, Y_train, GT_train, X_test, Y_test, GT_test = \\\n",
    "            load_brainMRI_dataset(dataset_path)\n",
    "    if args.ds == 'mvtec':\n",
    "        X_train, Y_train, GT_train, X_test, Y_test, GT_test, GT = \\\n",
    "            mvtec(5,dataset_path,15)\n",
    "\n",
    "    data_path = os.path.join('results','test_data', str(args.ds))\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    n_examples=len(X_train)\n",
    "    print(\"numero di esempi: \",n_examples)\n",
    "\n",
    "    np.save(open(os.path.join(data_path, 'X_train.npy'), 'wb'), X_train)\n",
    "    np.save(open(os.path.join(data_path, 'Y_train.npy'), 'wb'), Y_train)\n",
    "    np.save(open(os.path.join(data_path, 'GT_train.npy'), 'wb'), GT_train)\n",
    "\n",
    "    np.save(open(os.path.join(data_path, 'X_test.npy'), 'wb'), X_test)\n",
    "    np.save(open(os.path.join(data_path, 'Y_test.npy'), 'wb'), Y_test)\n",
    "    np.save(open(os.path.join(data_path, 'GT_test.npy'), 'wb'), GT_test)\n",
    "\n",
    "    np.save(open(os.path.join(data_path, 'GT.npy'), 'wb'), GT)\n",
    "    ret_path = os.path.join('results','output', str(args.ds))\n",
    "    if not os.path.exists(ret_path):\n",
    "        os.makedirs(ret_path)\n",
    "    np.save(open(os.path.join(ret_path, 'gt.npy'), 'wb'), GT_test)\n",
    "    np.save(open(os.path.join(ret_path, 'labels.npy'), 'wb'), Y_test)\n",
    "\n",
    "    pickle.dump(args, open(os.path.join(ret_path, 'args'), 'wb'))\n",
    "\n",
    "    times = []\n",
    "\n",
    "    print(\"sum: \", np.sum(Y_train == 0))\n",
    "\n",
    "    #lambda_u = n_examples / np.sum(Y_train == 0)\n",
    "    lambda_u = 1 / np.sum(Y_train == 0)\n",
    "    lambda_n=0\n",
    "    lambda_a=0\n",
    "\n",
    "    print(\"first lambda_u: \", lambda_u)\n",
    "\n",
    "    for x in range(0, b):\n",
    "\n",
    "        heatmaps, scores, _, _, tot_time = training_active_aexad(data_path,epochs=1,dataset=str(args.ds),\n",
    "                                                                 lambda_u = lambda_u, lambda_n = lambda_n, lambda_a = lambda_a)\n",
    "\n",
    "        active_images=os.path.join('results',\"query\",str(args.ds),str(x))\n",
    "        if not os.path.exists(active_images):\n",
    "            os.makedirs(active_images)\n",
    "\n",
    "        htmaps_aexad_conv = np.load(open(os.path.join(ret_path, 'aexad_htmaps.npy'), 'rb'))\n",
    "        scores_aexad_conv = np.load(open(os.path.join(ret_path, 'aexad_scores.npy'), 'rb'))\n",
    "\n",
    "        idx = np.argsort(scores_aexad_conv[Y_train == 0])[::-1]\n",
    "        img=\"a\"\n",
    "        ext=\".png\"\n",
    "\n",
    "        #query selection\n",
    "        image_to_save = X_train[Y_train==0][idx[0]]\n",
    "\n",
    "        img_to_save = Image.fromarray(image_to_save.astype(np.uint8))\n",
    "        img_to_save.save(os.path.join(active_images, img+ext))\n",
    "        print(\"dim image: \", image_to_save.shape)\n",
    "\n",
    "        mask_images=os.path.join('results',\"mask\",str(args.ds),str(x))\n",
    "        if not os.path.exists(mask_images):\n",
    "            os.makedirs(mask_images)\n",
    "\n",
    "        from_path=os.path.join(active_images,img+ext)\n",
    "        to_path=os.path.join(mask_images,img+\"_mask\"+ext)\n",
    "\n",
    "        #generazione della maschera manuale\n",
    "        #run_mask_generation(from_path,to_path)\n",
    "\n",
    "        #generazione della maschera dal dataset etichettato\n",
    "        mask_from_gt = GT[Y_train == 0][idx[0]]\n",
    "        mask_img = Image.fromarray(mask_from_gt.astype(np.uint8))\n",
    "        to_path = os.path.join(mask_images, img + \"_mask\" + ext)\n",
    "        mask_img.save(to_path)\n",
    "\n",
    "        #aggiornamento del dataset\n",
    "        mask_img = Image.open(to_path)\n",
    "        mask_array = np.array(mask_img)\n",
    "        print(\"dimensioni maschera: \", mask_array.shape)\n",
    "\n",
    "        #aggiornamento del dataset\n",
    "        X_train, Y_train, GT_train, lambda_u, lambda_n, lambda_a = \\\n",
    "            update_datasets(idx[0], mask_array, X_train, Y_train, GT_train)\n",
    "\n",
    "        np.save(open(os.path.join(data_path, 'X_train.npy'), 'wb'), X_train)\n",
    "        np.save(open(os.path.join(data_path, 'Y_train.npy'), 'wb'), Y_train)\n",
    "        np.save(open(os.path.join(data_path, 'GT_train.npy'), 'wb'), GT_train)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "9v9buf5oZ3zO",
    "outputId": "9041e722-357f-4347-ac52-69e9f073ef38",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/mvtec/hazelnut/train/good'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-aca9dbd03699>\u001B[0m in \u001B[0;36m<cell line: 58>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     74\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mds\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'mvtec'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mGT_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mGT_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mGT\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m             \u001B[0mmvtec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdataset_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[0mdata_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'results'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'test_data'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/active_ae_xad/aexad/tools/create_dataset.py\u001B[0m in \u001B[0;36mmvtec\u001B[0;34m(cl, path, n_anom_per_cls, seed)\u001B[0m\n\u001B[1;32m    248\u001B[0m     \u001B[0;31m# Add normal data to train set\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m     \u001B[0mf_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'train'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'good'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m     \u001B[0mnormal_files_tr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mfile\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnormal_files_tr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'png'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'jpg'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'npy'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'datasets/mvtec/hazelnut/train/good'"
     ]
    }
   ]
  }
 ]
}